{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "collapsed_sections": [
        "xihlLm_T6NFL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Análisis de sentimientos y clasificación de emociones en español**\n",
        "\n",
        "Este notebook describe el procedimiento para llevar a cabo un análisis automático de sentimientos (AS) y emociones (CE) en interacciones habladas en idioma español. El análisis se basa en la transcripción generada a partir de archivos de audio o video, y permite identificar tanto los sentimientos como las emociones expresadas por los participantes y durante toda la interacción."
      ],
      "metadata": {
        "id": "55ayuKnoiVdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configuración inicial\n",
        "\n",
        "A continuación, se detallan las librerías principales que se utilizarán y su propósito dentro del flujo de trabajo."
      ],
      "metadata": {
        "id": "SV_LiGkSiZnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalación de WhisperX\n",
        "\n",
        "**Whisper** es una librería de OpenAI para la transcripción de texto desde audio o video.\n",
        "\n",
        "[**WhisperX**](https://github.com/m-bain/whisperX) es una extensión de Whisper que se enfoca en mejorar la precisión de la transcripción utilizando un conjunto de modelos y técnicas adicionales. Esto incluye la alineación de la transcripción con el audio, la diarización para separar a los diferentes hablantes y la traducción a otros idiomas.\n"
      ],
      "metadata": {
        "id": "F-9JhYILibEU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCEi3Rv1iSu4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Instalación completa de WhisperX y dependencias necesarias para GPU\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install git+https://github.com/m-bain/whisperx.git@v3.3.1 -q\n",
        "!apt-get update\n",
        "!apt-get install libcudnn8=8.9.2.26-1+cuda12.1\n",
        "!apt-get install libcudnn8-dev=8.9.2.26-1+cuda12.1\n",
        "\n",
        "# Activar cálculos TF32 para mejorar rendimiento en GPU\n",
        "!python -c \"import torch; torch.backends.cuda.matmul.allow_tf32 = True; torch.backends.cudnn.allow_tf32 = True\"\n",
        "\n",
        "# Motor de inferencia rápido utilizado por WhisperX\n",
        "!pip install ctranslate2==4.4.0\n",
        "\n",
        "# Librería para diarización (identificación de hablantes)\n",
        "!pip install pyannote.audio==0.0.1\n",
        "\n",
        "# Traductor automático multilingüe (usado para traducir la transcripción)\n",
        "!pip install --upgrade translators"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalación de PySentimiento\n",
        "\n",
        "[**PySentimiento**](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis) es una librería que utiliza modelos pre-entrenados de transformers para distintas tareas de SocialNLP. Usa como modelos bases a BETO y RoBERTuito en Español, BERTweet en inglés, y otros modelos similares en italiano y portugués.\n",
        "\n",
        "Funcionalidades de Pysentimiento:\n",
        "\n",
        "  * *Análisis de sentimientos:* Detecta si el sentimiento en un texto es positivo, negativo o neutro.\n",
        "  * *Detección de emociones:* Identifica emociones específicas como alegría, tristeza, ira, etc.\n",
        "  * *Clasificación de opiniones:* Permite categorizar textos según opiniones o puntos de vista."
      ],
      "metadata": {
        "id": "INxC-nWsjSmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysentimiento"
      ],
      "metadata": {
        "id": "2BWYViWfjdJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###roberta-base-go_emotions\n",
        "\n",
        "[**roberta-base-go_emotions**](https://huggingface.co/SamLowe/roberta-base-go_emotions) es un modelo de clasificación de emociones basado en **RoBERTa**, entrenado sobre el conjunto de datos **GoEmotions** de Google.\n",
        "\n",
        "Este modelo permite detectar múltiples emociones simultáneamente en un texto (clasificación multi-etiqueta), y es especialmente útil para tareas de análisis emocional fino en **inglés**.\n",
        "\n",
        "#### Características principales:\n",
        "- Basado en **RoBERTa-base**, un modelo robusto para comprensión del lenguaje.\n",
        "- Entrenado sobre más de 58 mil frases con etiquetas de **27 emociones + neutral**.\n",
        "- Permite obtener una distribución de emociones como: *joy*, *anger*, *sadness*, *surprise*, *fear*, *disgust*, entre otras.\n",
        "- Soporta análisis multilabel: un mismo texto puede transmitir varias emociones a la vez.\n",
        "\n",
        "Este modelo complementa el análisis en español realizado con `pysentimiento`, y es útil para reevaluar o comparar resultados en textos traducidos al inglés.\n",
        "\n"
      ],
      "metadata": {
        "id": "JE5XscfDzKBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)"
      ],
      "metadata": {
        "id": "BkgqxOEazlD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pydub\n",
        "\n",
        "**Pydub** es una biblioteca de Python que facilita la manipulación de archivos de audio de forma simple e intuitiva.\n",
        "\n",
        "Se utiliza principalmente para tareas de procesamiento y edición de audio, sin necesidad de trabajar a bajo nivel con señales o buffers.\n",
        "\n",
        "Funcionalidades destacadas:\n",
        "- Carga y exportación de archivos en múltiples formatos (WAV, MP3, etc.).\n",
        "- Recorte de fragmentos de audio por tiempo.\n",
        "- Unión o superposición de varios audios.\n",
        "- Cambio de volumen, velocidad, canal o frecuencia de muestreo.\n",
        "- Conversión de formatos.\n",
        "\n",
        "En este proyecto, `pydub` se utiliza principalmente para obtener fragmentos de audio asociados a cada hablante, lo que facilita su posterior etiquetado manual."
      ],
      "metadata": {
        "id": "tG1wsZo6la8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "nRIsiy4_qvzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Deep-Translator\n",
        "\n",
        "**Deep-Translator** es una biblioteca que facilita la traducción automática utilizando distintos motores de traducción.\n",
        "\n",
        "En este proyecto se eligió utilizar **GoogleTranslator** por las siguientes ventajas:\n",
        "\n",
        "- No impone restricciones en la cantidad de caracteres a traducir.\n",
        "- No requiere una clave de API (*API-Key*), lo que simplifica su uso y configuración.\n",
        "- Proporciona traducciones rápidas y precisas para diversos idiomas.\n",
        "\n"
      ],
      "metadata": {
        "id": "fQ3JhnR20Mgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U deep-translator"
      ],
      "metadata": {
        "id": "COo5FLwzALnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNsmY44DCCfx"
      },
      "outputs": [],
      "source": [
        "from deep_translator import (GoogleTranslator,\n",
        "                             single_detection,\n",
        "                             batch_detection)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NLTK: Tokenización y manejo de *stopwords*\n",
        "\n",
        "**NLTK** (Natural Language Toolkit) es una biblioteca popular en Python para procesamiento de lenguaje natural.\n",
        "\n",
        "Esta preparación es fundamental para limpiar y procesar textos antes de aplicar análisis de sentimientos o emociones.\n"
      ],
      "metadata": {
        "id": "jHAJlMRJ08Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "sPVzKoft1Ju5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imports\n",
        "\n",
        "En este notebook utilizaremos diversas librerías y módulos para realizar el análisis automático de sentimientos. Algunas de estas librerías necesitan ser instaladas previamente, mientras que otras ya vienen preinstaladas en el entorno de Google Colab y sólo es necesario importarlas.\n"
      ],
      "metadata": {
        "id": "glwkmlGsjP-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesamiento y transcripción de audio/video con aceleración en GPU\n",
        "import torch\n",
        "import whisperx\n",
        "\n",
        "# Manejo y análisis de datos estructurados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualización gráfica y generación de gráficos\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Manipulación y comparación avanzada de texto\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Conteo eficiente de elementos (palabras, tokens)\n",
        "from collections import Counter\n",
        "\n",
        "# Gestión manual de memoria para optimizar recursos\n",
        "import gc\n",
        "\n",
        "# Analizador de sentimientos y emociones en español\n",
        "from pysentimiento import create_analyzer\n",
        "\n",
        "# Uso de modelos preentrenados para tareas de NLP\n",
        "import transformers\n",
        "\n",
        "# Generación de nubes de palabras para visualización\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Interacción con archivos y sistema operativo en Colab\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Manipulación de segmentos de audio\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Reproducción integrada de audio en notebooks\n",
        "from IPython.display import Audio, display\n"
      ],
      "metadata": {
        "id": "yfzBZn__jHKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "bxE3F0Xly-RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Token Hugging Face\n",
        "\n",
        "Para usar esta herramienta, necesitas un token de acceso de Hugging Face. Puedes obtenerlo en:  [Token de HF](https://huggingface.co/docs/hub/security-tokens)"
      ],
      "metadata": {
        "id": "wfgrenN9jzPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = \"\" # Pega tu token aquí (ej. hf_XXXXXXXXXXXXXXXXXXXXXX)"
      ],
      "metadata": {
        "id": "8t_VV6n_j18F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Carga y Preprocesamiento del Archivo de Audio/Video\n",
        "\n",
        "Para realizar el análisis, primero necesitamos cargar el archivo que deseamos procesar. Para ello, ejecuta la siguiente celda y selecciona el archivo correspondiente.\n",
        "\n",
        "**Importante:** Solo se puede cargar un archivo a la vez, y este debe tener una de las siguientes extensiones: **.mp3**, **.mp4** o **.wav**. De lo contrario, la carga no se realizará correctamente.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jX4PYkkGjtRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir archivo de audio o video\n",
        "uploaded = files.upload()\n",
        "\n",
        "if len(uploaded) == 1:\n",
        "    filename = next(iter(uploaded.keys()))\n",
        "\n",
        "    # Verifica la extensión del archivo\n",
        "    valid_extensions = ['.wav', '.mp3', '.mp4']\n",
        "    file_extension = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "    if file_extension in valid_extensions:\n",
        "        print(f'\\nArchivo válido subido: {filename}')\n",
        "    else:\n",
        "        print(f'\\nError: La extensión del archivo {filename} no es válida. El archivo será eliminado.')\n",
        "        os.remove(filename)  # Elimina el archivo subido si no es válido\n",
        "else:\n",
        "    print(f'\\nError: Debes subir solo un archivo. Se eliminarán todos los archivos subidos.')\n",
        "    # Eliminar todos los archivos subidos\n",
        "    for filename in uploaded.keys():\n",
        "        os.remove(filename)"
      ],
      "metadata": {
        "id": "cBcATVX9rA6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcripción de Texto con WhisperX\n",
        "\n",
        "Cargar modelo WhisperX (ajustar a tus necesidades: tiny, base, medium, large)\n",
        "\n",
        "WhisperX utiliza los mismos modelos de Whisper de OpenAI y agrega alineación precisa de palabras.\n",
        "\n",
        "Los modelos disponibles son:\n",
        "\n",
        "* **Tiny**: Muy rápido, bajo consumo de recursos, precisión limitada.\n",
        "* **Base**: Rápido, buen rendimiento en inglés, adecuado para tareas simples.\n",
        "* **Small**: Balance entre rapidez y precisión, mejor soporte multilingüe.\n",
        "* **Medium**: Alta precisión en múltiples idiomas, más lento, ideal para audio ruidoso.\n",
        "* **Large**: Máxima precisión, ideal para tareas exigentes, pero lento y pesado."
      ],
      "metadata": {
        "id": "pUIjotnaoV4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "batch_size = 8 # Reducir si hay poca memoria GPU.\n",
        "compute_type = \"float16\" # Cambiar a \"int 8\" si hay poca GPU (posiblemente reduzca la presición)"
      ],
      "metadata": {
        "id": "OQmgdSCXoh_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, asr_options={'hotwords': []})"
      ],
      "metadata": {
        "id": "qglqwC5Nucr0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos la siguiente celda para realizar la transcripción y alineación de los segmentos"
      ],
      "metadata": {
        "id": "-dzE6d9q8jFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribir el audio\n",
        "audio = whisperx.load_audio(filename)\n",
        "result = model.transcribe(audio, batch_size=batch_size)\n",
        "\n",
        "# Alinear segmentos con un modelo de alineamiento adicional\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "#Guarda la transcripcion en un archivo .txt\n",
        "transcription_text = \"\\n\".join([segment['text'] for segment in result[\"segments\"]])\n",
        "with open('transcription.txt', 'w', encoding='utf-8') as file: # Modified line: explicitly use utf-8 encoding.\n",
        "  file.write(transcription_text)\n",
        "\n",
        "# Mostramos las marcas temporales y el texto alineado\n",
        "for segmento in result[\"segments\"]:\n",
        "    print(f\"[{segmento['start']} - {segmento['end']}] {segmento['text']}\")"
      ],
      "metadata": {
        "id": "r8gb8q9W7QyS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si desea descargar la transcripción como un archivo de texto, ejecutar la siguiente celda."
      ],
      "metadata": {
        "id": "bfsxcjBW8lw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('transcription.txt')"
      ],
      "metadata": {
        "id": "2KYr9P068its"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diarización\n",
        "\n",
        "**Definición:** La diarización es el proceso de identificar y separar las voces de los diferentes hablantes en una grabación de audio o en un flujo de video."
      ],
      "metadata": {
        "id": "NYCoVutv9vIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE:**  Para realizar la diarización de manera precisa, es fundamental especificar correctamente la cantidad de hablantes presentes en el archivo de audio. Proporcionar un número incorrecto puede llevar a errores en la asignación de intervenciones por parte de la herramienta **WhisperX**, lo que afecta negativamente la calidad del análisis.\n",
        "\n",
        "No obstante, en caso de que se desconozca la cantidad exacta de participantes, **WhisperX** puede estimarla automáticamente durante el proceso de análisis, sin interrumpir la ejecución."
      ],
      "metadata": {
        "id": "GqOo8gA4-Gee"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2xWRYz62u8l"
      },
      "source": [
        "Si conoce el número exacto de participantes, cambie el número y ejecute la siguiente línea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsyaOHpu4pVw"
      },
      "outputs": [],
      "source": [
        "numero_participantes = 2 #cambiar el número según corresponda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Asignar etiquetas de speaker (orador)\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
        "diarize_segments = diarize_model(audio, num_speakers=numero_participantes)\n",
        "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "#Lista de segmentos de audio con información de hablante y texto (y lo guardamos en un archivo .txt).\n",
        "with open('transcription.txt', 'w', encoding='utf-8') as file:\n",
        "  for segment in result[\"segments\"]:\n",
        "    if 'speaker' in segment:\n",
        "      speaker = segment['speaker']\n",
        "      text = segment['text'].strip()\n",
        "      if text:\n",
        "        file.write(f\"{speaker}: {text}\\n\")"
      ],
      "metadata": {
        "id": "Vf9wguSxTFmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En caso contrario, ejecutar la siguiente línea de código"
      ],
      "metadata": {
        "id": "Eo1wshQJ4uVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Asignar etiquetas de speaker (orador)\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
        "diarize_segments = diarize_model(audio)\n",
        "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "#Lista de segmentos de audio con información de hablante y texto (y lo guardamos en un archivo .txt).\n",
        "with open('transcription.txt', 'w', encoding='utf-8') as file:\n",
        "  for segment in result[\"segments\"]:\n",
        "    if 'speaker' in segment:\n",
        "      speaker = segment['speaker']\n",
        "      text = segment['text'].strip()\n",
        "      if text:\n",
        "        file.write(f\"{speaker}: {text}\\n\")"
      ],
      "metadata": {
        "id": "RW7B6pxo4xIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al ejecutar la siguiente celda, podremos observar cómo se clasificó cada segmento de texto según los participantes (speakers) correspondientes."
      ],
      "metadata": {
        "id": "OjdQgtRAALJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diarize_segments"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xOc8QbzwAJ4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si desea descargar la transcripción diarizada como un archivo de texto, ejecutar la siguiente celda."
      ],
      "metadata": {
        "id": "gZzCFfJwBNC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMYtCjlqb5rN"
      },
      "outputs": [],
      "source": [
        "files.download('transcription.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un DataFrame con los resultados obtenidos\n",
        "\n",
        "Un **DataFrame** de *pandas* es una forma de representar y trabajar con datos tabulares."
      ],
      "metadata": {
        "id": "ktj9JtI7B7Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audioDF = pd.DataFrame(result[\"segments\"])\n",
        "audioDF.head()"
      ],
      "metadata": {
        "id": "HddV6nAsB6Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O786LDCmTkgQ"
      },
      "source": [
        "Con el fin de mejorar el AS y CE en el texto, agrupamos las filas consecutivas del mismo hablante en el DataFrame, consolidando el texto, los tiempos de inicio y fin, y la lista de palabras en una única oración por hablante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14dMv2iOZGc1"
      },
      "outputs": [],
      "source": [
        "def group_consecutive_speakers(audioDF):\n",
        "    \"\"\"\n",
        "    Groups consecutive speaker segments into single sentences, adjusting 'end' time and 'words'.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(audioDF, pd.DataFrame) or \\\n",
        "       not all(col in audioDF.columns for col in ['speaker', 'start', 'end', 'text', 'words']):\n",
        "        raise ValueError(\"Input must be a pandas DataFrame with 'speaker', 'start', 'end', 'text', and 'words' columns.\")\n",
        "\n",
        "    grouped_segments = []\n",
        "    current_speaker = None\n",
        "    current_start = None\n",
        "    current_end = None\n",
        "    current_text = \"\"\n",
        "    current_words = []\n",
        "\n",
        "    for _, row in audioDF.iterrows():\n",
        "        text = row['text'].strip()\n",
        "        if row['speaker'] != current_speaker:\n",
        "            if current_speaker is not None:\n",
        "                grouped_segments.append({\n",
        "                    'speaker': current_speaker,\n",
        "                    'start': current_start,\n",
        "                    'end': current_end,\n",
        "                    'text': current_text.strip(),\n",
        "                    'words': current_words\n",
        "                })\n",
        "            current_speaker = row['speaker']\n",
        "            current_start = row['start']\n",
        "            current_end = row['end']\n",
        "            current_text = text\n",
        "            current_words = row['words']\n",
        "        else:\n",
        "            current_end = row['end']\n",
        "            current_text += \" \" + text\n",
        "            current_words.extend(row['words'])\n",
        "\n",
        "    # Agrega el último segmento\n",
        "    if current_speaker is not None:\n",
        "        grouped_segments.append({\n",
        "            'speaker': current_speaker,\n",
        "            'start': current_start,\n",
        "            'end': current_end,\n",
        "            'text': current_text.strip(),\n",
        "            'words': current_words\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(grouped_segments)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audioDF = group_consecutive_speakers(audioDF)\n",
        "audioDF.head()"
      ],
      "metadata": {
        "id": "opGKsTeLE7_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si desea descargar el **dataframe** como un archivo .csv, ejecutar la siguiente celda"
      ],
      "metadata": {
        "id": "VrOdgYv3CV0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el dataframe como un archivo CSV\n",
        "audioDF.to_csv('transcription_segments.csv', index=False)\n",
        "# Descargar el archivo CSV\n",
        "files.download('transcription_segments.csv')"
      ],
      "metadata": {
        "id": "uWAtcG6nCcJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiquetar Participantes\n",
        "\n"
      ],
      "metadata": {
        "id": "JhV26d2tr3mK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el DataFrame **audioDF**, podemos ver que los participantes están clasificados con etiquetas como \"SPEAKER_00\", \"SPEAKER_01\", etc.\n",
        "\n",
        "Para reemplazar estas etiquetas por los nombres reales de los participantes, primero ejecutaremos la siguiente celda, que generará un fragmento de audio correspondiente a cada hablante, facilitando su identificación auditiva.\n",
        "\n",
        "**Aclaración:** Si se desea renombrar a los speakers, es necesario ejecutar toda esta sección."
      ],
      "metadata": {
        "id": "OPuzz14asntH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set para registrar oradores ya procesados\n",
        "processed_speakers = set()\n",
        "# Lista para almacenar los oradores en orden de procesamiento\n",
        "orators = []\n",
        "\n",
        "# Cargar el archivo de audio completo\n",
        "audio = AudioSegment.from_file(filename)\n",
        "\n",
        "# Crear lista temporal: índice, orador, inicio, fin, duración\n",
        "temp_list = [\n",
        "    (idx, row['speaker'], row['start'], row['end'], row['end'] - row['start'])\n",
        "    for idx, row in audioDF.iterrows()\n",
        "]\n",
        "\n",
        "# Buscar el primer segmento > 2 segundos por orador\n",
        "seen = set()\n",
        "filtered_segments = []\n",
        "for idx, speaker, start, end, duration in temp_list:\n",
        "    if speaker not in seen and duration > 2: #Modificar si se desea\n",
        "        seen.add(speaker)\n",
        "        filtered_segments.append((idx, speaker, start, end, duration))\n",
        "\n",
        "# Procesar y reproducir los segmentos\n",
        "for idx, speaker, start, end, duration in filtered_segments:\n",
        "    if speaker not in processed_speakers:\n",
        "        processed_speakers.add(speaker)\n",
        "\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "\n",
        "        segment = audio[start_ms:end_ms]\n",
        "        segment.export(f\"temp_segment_{speaker}.wav\", format=\"wav\")\n",
        "\n",
        "        print(f\"\\nReproduciendo audio para {speaker}...\")\n",
        "        display(Audio(f\"temp_segment_{speaker}.wav\"))\n",
        "\n",
        "        orators.append(speaker)\n"
      ],
      "metadata": {
        "id": "G-zvFNWVovWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez que hemos identificado a los participantes a través de los audios, ejecuta la siguiente celda y completá según corresponda.\n",
        "\n",
        "Para guardar los cambios, presiona **Enter**."
      ],
      "metadata": {
        "id": "oNNcVJ4IsuBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orator_names = {}\n",
        "\n",
        "for speaker in orators:\n",
        "    # Solicitar el nombre del orador al usuario\n",
        "    speaker_name = input(f\"Ingrese el nombre del orador para {speaker}: \")\n",
        "\n",
        "    # Guardar el nombre del orador en un diccionario\n",
        "    orator_names[speaker] = speaker_name\n",
        "\n",
        "# Actualizar el DataFrame con los nuevos nombres de los oradores\n",
        "for index, row in audioDF.iterrows():\n",
        "    speaker = row['speaker']\n",
        "\n",
        "    if speaker in orator_names:\n",
        "        # Cambiar el nombre del orador en el DataFrame\n",
        "        audioDF.at[index, 'speaker'] = orator_names[speaker]"
      ],
      "metadata": {
        "id": "aEBOi-GupZr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirmamos que el cambio se haya realizado correctamente."
      ],
      "metadata": {
        "id": "QjlfMuau5Wuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audioDF.head()"
      ],
      "metadata": {
        "id": "wwh5J239sjpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrar StopWords\n",
        "\n",
        "Las stopwords son palabras de uso frecuente, como \"el\", \"la\", \"un\", \"una\" o \"en\", que suelen ser omitidas en tareas de procesamiento de lenguaje natural, ya que aportan poca información semántica relevante para el análisis textual o la recuperación de información.\n",
        "\n",
        "Además, se descartan las oraciones que contienen menos de 4 tokens, quedándonos únicamente con aquellas que tienen 4 o más tokens para asegurar un análisis más significativo.\n",
        "\n"
      ],
      "metadata": {
        "id": "xihlLm_T6NFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir stopwords\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Puntuación a eliminar (excepto signos de exclamación/interrogación)\n",
        "punctuation_to_strip = set(string.punctuation) | {\"«\", \"»\", \"…\"}\n",
        "punctuation_to_strip -= {\"¡\", \"¿\", \"!\", \"?\"}\n",
        "\n",
        "def remove_stopwords_es(text):\n",
        "    # Tokenizamos\n",
        "    tokens = word_tokenize(text.lower(), language='spanish')\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        # Quitar puntuación innecesaria\n",
        "        word_clean = token.strip(\"\".join(punctuation_to_strip))\n",
        "\n",
        "        # Conservar si no es stopword ni vacío\n",
        "        if word_clean and word_clean not in stop_words:\n",
        "            cleaned_tokens.append(word_clean)\n",
        "\n",
        "    # Reconstruir texto y unir signos sueltos a la palabra anterior\n",
        "    texto = ' '.join(cleaned_tokens)\n",
        "\n",
        "    # Quitar espacios antes de los signos de admiración/interrogación\n",
        "    texto = re.sub(r'\\s+([?!¡¿])', r'\\1', texto)\n",
        "\n",
        "    return texto"
      ],
      "metadata": {
        "id": "KhFucQ5A6bbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audioDF['duration'] = audioDF['end'] - audioDF['start']\n",
        "\n",
        "audioDF[\"text_clean\"] = audioDF[\"text\"].apply(remove_stopwords_es)\n",
        "audioDF = audioDF[audioDF['text_clean'].str.strip().astype(bool)]\n",
        "audioDF = audioDF[audioDF['text_clean'].apply(lambda x: len(str(x).split()) >= 4)]\n",
        "audioDF.head()"
      ],
      "metadata": {
        "id": "qdcKrzKz6gF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AS y CE"
      ],
      "metadata": {
        "id": "ukIyDZ-lBQ9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la siguiente casilla podrá indicar el idioma del audio/video a analizar.\n",
        "\n",
        "Nuestro modelo está especialmente diseñado para realizar el análisis en **español**, que es el idioma configurado por defecto."
      ],
      "metadata": {
        "id": "Rq2yeFSkMOUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenguaje = \"es\" # Idioma del audio a analizar. Para modificarlo, cambie lo que está dentro de las comillas por \"es\" (sin las comillas) cuando el idioma es español o \"en\" cuando es en inglés"
      ],
      "metadata": {
        "id": "2okjKs5WMJZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente celda va a realizar el **análisis de sentimientos y de emociones** a cada frase que se ha transcripto anteriormente gracias a WhisperX."
      ],
      "metadata": {
        "id": "tMPQXTIYNSxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=lenguaje)\n",
        "emotion_analyzer = create_analyzer(task=\"emotion\", lang=lenguaje)\n",
        "\n",
        "audioDF['sentimiento'] = audioDF['text_clean'].apply(lambda text: sentiment_analyzer.predict(text).output)\n",
        "audioDF['emocion'] = audioDF['text_clean'].apply(lambda text: emotion_analyzer.predict(text).output)\n",
        "\n",
        "audioDF['sentimiento_score'] = audioDF['text_clean'].apply(lambda text: sentiment_analyzer.predict(text).probas)\n",
        "audioDF['emocion_score'] = audioDF['text_clean'].apply(lambda text: emotion_analyzer.predict(text).probas)"
      ],
      "metadata": {
        "id": "4l8Ajf7SBQid",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta manera, gracias a Pysentimiento, tenemos en nuestro DataFrame cada frase con su **sentimiento** y su **emoción** respectivamente.\n",
        "\n",
        "En la siguiente celda podremos observar los sentimientos y emociones por interacción de cada participante"
      ],
      "metadata": {
        "id": "IiXZ6VwIlsRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audioDF[['text','speaker','sentimiento','emocion']]"
      ],
      "metadata": {
        "id": "U2o4XNFClK_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis complementario en inglés\n",
        "\n",
        "Para optimizar el AS y CE, se traducirán al inglés los textos que **PySentimiento** haya clasificado con la emoción *'others'* o el sentimiento *'NEU'*, y se aplicará un análisis complementario utilizando los modelos **PySentimiento** en inglés y **roberta-base-go_emotions**.\n",
        "\n",
        "La ventaja principal de este último modelo radica en su capacidad para clasificar hasta 26 emociones, en contraste con *PySentimiento*, que se limita a las 6 emociones básicas.\n",
        "\n",
        "**Importante**: ejecutar esta sección solo si el DataFrame **auxDF** no está vacío."
      ],
      "metadata": {
        "id": "tV_R6Sj_7P99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traducimos solo los textos cuya emocion es 'others' o el sentimiento 'neutral'\n",
        "# Para no perder datos, usamos un DataFrame auxiliar con esos casos\n",
        "\n",
        "auxDF = audioDF[(audioDF['emocion'] == 'others') | (audioDF['sentimiento'] == 'NEU')]\n",
        "auxDF"
      ],
      "metadata": {
        "id": "br-PYjK97VIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Traducción con Deep-Translator**\n",
        "\n",
        "Esta herramienta ofrece compatibilidad con múltiples motores de traducción. En este caso, se optó por utilizar **GoogleTranslator**"
      ],
      "metadata": {
        "id": "5vy4c3_87mHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dFr_KPvBCZ59"
      },
      "outputs": [],
      "source": [
        "def traducir(texto):\n",
        "    try:\n",
        "        return GoogleTranslator(source='auto', target='en').translate(texto)\n",
        "    except Exception as e:\n",
        "        print(f\"Error traduciendo: {texto} -> {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auxDF['text_translated'] = auxDF['text'].apply(traducir)\n",
        "auxDF.head()"
      ],
      "metadata": {
        "id": "oEjQBjwlh8TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Filtrar Stopwords\n",
        "\n",
        "Se eliminan las **stopwords** del texto previamente traducido al inglés con el objetivo de reducir el ruido y mejorar la calidad del análisis semántico.\n",
        "\n",
        "Se conserva el umbral mínimo de **4 tokens** tras la traducción y el filtrado, a fin de mantener la coherencia metodológica y la estabilidad de los resultados.\n"
      ],
      "metadata": {
        "id": "OWPB56VECFmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Puntuación a eliminar (excepto signos de exclamación/interrogación)\n",
        "punctuation_to_strip = set(string.punctuation) | {\"«\", \"»\", \"…\"}\n",
        "punctuation_to_strip -= {\"¡\", \"¿\", \"!\", \"?\"}\n",
        "\n",
        "def remove_stopwords_en(text):\n",
        "    # Tokenizamos\n",
        "    tokens = word_tokenize(text.lower(), language='english')\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        # Quitar puntuación innecesaria\n",
        "        word_clean = token.strip(\"\".join(punctuation_to_strip))\n",
        "\n",
        "        # Conservar si no es stopword ni vacío\n",
        "        if word_clean and word_clean not in stop_words:\n",
        "            cleaned_tokens.append(word_clean)\n",
        "\n",
        "    # Reconstruir texto y unir signos sueltos a la palabra anterior\n",
        "    texto = ' '.join(cleaned_tokens)\n",
        "\n",
        "    # Quitar espacios antes de los signos de admiración/interrogación\n",
        "    texto = re.sub(r'\\s+([?!¡¿])', r'\\1', texto)\n",
        "\n",
        "    return texto"
      ],
      "metadata": {
        "id": "vzp_P1out9mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auxDF[\"text_clean_en\"] = auxDF[\"text_translated\"].apply(remove_stopwords_en)\n",
        "auxDF = auxDF[auxDF['text_clean_en'].apply(lambda x: len(str(x).split()) >= 4)]\n",
        "auxDF.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hXVzUgmhVBzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_QJomiUP2aT"
      },
      "source": [
        "###CE con roberta-base-go_emotions\n",
        "\n",
        "Realizamos la CE de las frases traducidas con el modelo **\"roberta-base-go_emotions\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KyQvjsnR5if"
      },
      "outputs": [],
      "source": [
        "def analisis_baseGoEmotions_samlowe(text):\n",
        "    model_outputs = classifier([text])  # <- Usar el texto recibido\n",
        "    return model_outputs[0][0]['label']  # Retorna la emoción con mayor score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auxDF['emotion'] = auxDF['text_translated'].apply(analisis_baseGoEmotions_samlowe)\n",
        "auxDF.head()"
      ],
      "metadata": {
        "id": "s81Bu0YniGgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpa_UKyvKaBF"
      },
      "source": [
        "El modelo de CE utilizado está entrenado con el dataset **GoEmotions** de Google, el cual permite identificar una gama más amplia de emociones que las seis emociones básicas que devuelve **PySentimiento** en español. Para hacer comparables los resultados, aplicaremos el esquema de agrupación propuesto en el [paper de GoEmotions](https://arxiv.org/pdf/2005.00547), mapeando las emociones del modelo al conjunto de seis emociones básicas utilizadas por PySentimiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppV66L7rMLIj"
      },
      "outputs": [],
      "source": [
        "mapping = {\n",
        "    # anger group\n",
        "    'anger': 'anger',\n",
        "    'annoyance': 'anger',\n",
        "    'disapproval': 'anger',\n",
        "\n",
        "    # disgust group\n",
        "    'disgust': 'disgust',\n",
        "\n",
        "    # fear group\n",
        "    'fear': 'fear',\n",
        "    'nervousness': 'fear',\n",
        "\n",
        "    # joy group\n",
        "    'admiration': 'joy',\n",
        "    'amusement': 'joy',\n",
        "    'approval': 'joy',\n",
        "    'caring': 'joy',\n",
        "    'desire': 'joy',\n",
        "    'excitement': 'joy',\n",
        "    'gratitude': 'joy',\n",
        "    'joy': 'joy',\n",
        "    'love': 'joy',\n",
        "    'pride': 'joy',\n",
        "    'optimism': 'joy',\n",
        "    'relief': 'joy',\n",
        "\n",
        "    # sadness group\n",
        "    'sadness': 'sadness',\n",
        "    'disappointment': 'sadness',\n",
        "    'embarrassment': 'sadness',\n",
        "    'grief': 'sadness',\n",
        "    'remorse': 'sadness',\n",
        "\n",
        "    # surprise group\n",
        "    'confusion': 'surprise',\n",
        "    'curiosity': 'surprise',\n",
        "    'realization': 'surprise',\n",
        "    'surprise': 'surprise',\n",
        "\n",
        "    # neutral group\n",
        "    'neutral': 'neutral'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLRBTP8eMXSq"
      },
      "outputs": [],
      "source": [
        "auxDF.loc[:, 'emotion'] = auxDF['emotion'].map(mapping).fillna('neutral')\n",
        "auxDF.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análisis de Sentimiento con PySentimiento en inglés"
      ],
      "metadata": {
        "id": "2DDF_7A_4AnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqIOrrCLt8Q9"
      },
      "outputs": [],
      "source": [
        "lenguaje = \"en\" # Idioma del audio a analizar. Para modificarlo, cambie lo que está dentro de las comillas por \"es\" (sin las comillas) cuando el idioma es español o \"en\" cuando es en inglés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ff2TXT3ct8Q9"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer_en = create_analyzer(task=\"sentiment\", lang=lenguaje)\n",
        "\n",
        "auxDF['sentiment'] = auxDF['text_clean_en'].apply(lambda text: sentiment_analyzer_en.predict(text).output)\n",
        "auxDF.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0fxl472pQlP"
      },
      "source": [
        "### Reemplazo de los sentimientos y emociones\n",
        "\n",
        "Se reemplazan los valores de la columna \"sentimiento\" que son iguales a \"NEU\" por los correspondientes de la columna \"sentiment\". De manera análoga, los valores \"others\" en la columna \"emocion\" se sustituyen por los valores equivalentes presentes en la columna \"emotion\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reemplazos condicionales\n",
        "auxDF.loc[auxDF[\"sentimiento\"] == \"NEU\", \"sentimiento\"] = auxDF[\"sentiment\"]\n",
        "auxDF.loc[auxDF[\"emocion\"] == \"others\", \"emocion\"] = auxDF[\"emotion\"]\n",
        "\n",
        "#Eliminamos las columnas\n",
        "auxDF = auxDF.drop(['sentiment','emotion'], axis=1)\n",
        "\n",
        "# Renombrar columnas\n",
        "auxDF = auxDF.rename(columns={\n",
        "    \"sentimiento\": \"sentiment\",\n",
        "    \"emocion\": \"emotion\"\n",
        "})"
      ],
      "metadata": {
        "id": "H1lmr0YJavPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auxDF.head()"
      ],
      "metadata": {
        "id": "zRpNKcL36DY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unimos los datos obtenidos en este análisis con los originales"
      ],
      "metadata": {
        "id": "XWrI_TDRDIeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auxDF = auxDF[['text','sentiment','emotion']]\n",
        "\n",
        "audioDF = audioDF.merge(auxDF, how='left', on='text')\n",
        "audioDF"
      ],
      "metadata": {
        "id": "cVoY8nSKa6VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reemplazamos los valores originales por los nuevos"
      ],
      "metadata": {
        "id": "ryy_0TwDDVTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reemplazar 'sentimiento' solo si es 'NEU' y 'sentiment' no es NaN\n",
        "audioDF.loc[\n",
        "    (audioDF[\"sentimiento\"] == \"NEU\") & audioDF[\"sentiment\"].notna(),\n",
        "    \"sentimiento\"\n",
        "] = audioDF[\"sentiment\"]\n",
        "\n",
        "# Reemplazar 'emocion' solo si es 'others' y 'emotion' no es NaN\n",
        "audioDF.loc[\n",
        "    (audioDF[\"emocion\"] == \"others\") & audioDF[\"emotion\"].notna(),\n",
        "    \"emocion\"\n",
        "] = audioDF[\"emotion\"]\n",
        "\n",
        "audioDF.loc[audioDF[\"emocion\"] == \"others\", \"emocion\"] = \"neutral\"\n",
        "audioDF = audioDF.drop(['sentiment','emotion'], axis=1)\n",
        "\n",
        "audioDF"
      ],
      "metadata": {
        "id": "_siPkerwARLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soCK0BAiVZYs"
      },
      "source": [
        "##Gráficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdE41MVLQ5bT"
      },
      "outputs": [],
      "source": [
        "#Traducir las emociones\n",
        "audioDF['emocion'] = audioDF['emocion'].replace({\n",
        "    'joy': 'alegría',\n",
        "    'surprise': 'sorpresa',\n",
        "    'disgust': 'disgusto',\n",
        "    'sadness': 'tristeza',\n",
        "    'fear': 'miedo',\n",
        "    'anger': 'enojo'\n",
        "})\n",
        "\n",
        "colores_sentimientos = {\n",
        "    'POS': '#00A651',\n",
        "    'NEG': '#F05A61',\n",
        "    'NEU':  '#FFCB08'\n",
        "}\n",
        "\n",
        "colores_emociones = {\n",
        "    'enojo': '#F05A61',\n",
        "    'alegría': '#FFCB08',\n",
        "    'tristeza': '#2884C6',\n",
        "    'miedo': '#00A651',\n",
        "    'sorpresa': '#009ACE',\n",
        "    'disgusto':  '#8A73B3',\n",
        "    'neutral':  '#F7923D',\n",
        "    'others':  '#F7923D'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YEvPwjBxSUa"
      },
      "source": [
        "###Gráfico de duración por orador"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Grafica la duración por orador\n",
        "speaker_durations = audioDF.groupby('speaker')['start'].sum()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(speaker_durations.index, speaker_durations.values)\n",
        "plt.xlabel('Orador')\n",
        "plt.ylabel('Duración total (segundos)')\n",
        "plt.title('Duración de la conversación por orador')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QRixB-_N9Msf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsIPuTyfxSUc"
      },
      "source": [
        "###Gráfico de sentimientos por orador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf_VuUpipSq9"
      },
      "outputs": [],
      "source": [
        "sentiment_durations = audioDF.groupby([\"speaker\", \"sentimiento\"])[\"duration\"].sum().unstack(fill_value=0)\n",
        "\n",
        "# Calcular porcentajes por orador\n",
        "sentiments_percentages = sentiment_durations.div(sentiment_durations.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Graficar barras apiladas\n",
        "ax = sentiments_percentages.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(10, 6),\n",
        "    color=[colores_sentimientos[col] for col in sentiment_durations.columns]\n",
        ")\n",
        "\n",
        "# Personalización\n",
        "plt.xlabel('Orador')\n",
        "plt.ylabel('Porcentaje (%)')\n",
        "plt.title('Distribución de Sentimientos por Orador (según tiempo hablado)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Sentimientos\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Agregar etiquetas a la derecha de cada bloque\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height > 0:\n",
        "        ax.annotate(f'{height:.1f}%',\n",
        "                    (p.get_x() + p.get_width(), p.get_y() + height / 2),\n",
        "                    ha='left', va='center', fontsize=10, color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"sentimiento_predominante_orador.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afWGDpt_xSUd"
      },
      "source": [
        "###Gráfico de emociones por orador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izl0WrlzxSUe"
      },
      "outputs": [],
      "source": [
        "emotions_durations = audioDF.groupby([\"speaker\", \"emocion\"])[\"duration\"].sum().unstack(fill_value=0)\n",
        "emotions_percentages = emotions_durations.div(emotions_durations.sum(axis=1), axis=0) * 100\n",
        "\n",
        "ax = emotions_percentages.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(10, 6),\n",
        "    color=[colores_emociones[col] for col in emotions_durations.columns]\n",
        ")\n",
        "\n",
        "# Personalización\n",
        "plt.xlabel('Orador')\n",
        "plt.ylabel('Porcentaje (%)')\n",
        "plt.title('Distribución de Emociones por Orador (según tiempo hablado)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Emocion\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Etiquetas a la derecha de la barra\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height > 0:\n",
        "        ax.annotate(f'{height:.1f}%',\n",
        "                    (p.get_x() + p.get_width(), p.get_y() + height / 2),\n",
        "                    ha='left', va='center', fontsize=10, color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"emocion_predominante_orador.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmBsMB-bxSUf"
      },
      "source": [
        "###Grafico de sentimientos durante toda la interacción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBicn3gGs5tD"
      },
      "outputs": [],
      "source": [
        "sentiment_durations = audioDF.groupby(\"sentimiento\")[\"duration\"].sum()\n",
        "sentiment_percentages = (sentiment_durations / sentiment_durations.sum()) * 100\n",
        "\n",
        "colors = [colores_sentimientos.get(sent, \"#CCCCCC\") for sent in sentiment_percentages.index]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sentiment_percentages, labels=sentiment_percentages.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
        "\n",
        "plt.title(\"Distribución de Sentimientos en toda la interacción\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"sentimiento_predominante_toda_interaccion.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdfPUUklxSUg"
      },
      "source": [
        "###Gráfico de emociones durante toda la interacción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEaUhKC6tG-5"
      },
      "outputs": [],
      "source": [
        "emotions_durations = audioDF.groupby(\"emocion\")[\"duration\"].sum()\n",
        "emotions_percentages = (emotions_durations / emotions_durations.sum()) * 100\n",
        "\n",
        "colors = [colores_emociones.get(sent, \"#CCCCCC\") for sent in emotions_percentages.index]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(emotions_percentages, labels=emotions_percentages.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
        "\n",
        "plt.title(\"Distribución de Emociones en toda la interacción\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"emocion_predominante_toda_interaccion.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OrLtvK0xSUh"
      },
      "source": [
        "###Evolución de los sentimientos a lo largo del tiempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exoIX0hyJS21"
      },
      "outputs": [],
      "source": [
        "sentimiento_map = {'NEG': -1, 'NEU': 0, 'POS': 1}\n",
        "audioDF['sentimiento_num'] = audioDF['sentimiento'].map(sentimiento_map)\n",
        "audioDF['mid_time'] = (audioDF['start'] + audioDF['end']) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMR7rdq1JS21"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=audioDF, x='mid_time', y='sentimiento_num', hue='speaker', marker='o')\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
        "plt.title('Sentimiento a lo largo del tiempo por participante')\n",
        "plt.xlabel('Tiempo (s)')\n",
        "plt.ylabel('Sentimiento')\n",
        "plt.yticks([-1, 0, 1], ['Negativo', 'Neutral', 'Positivo'])\n",
        "plt.legend(title='Orador')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"sentimiento_tiempo_speakers.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKWyMnbuJS21"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "sns.lineplot(data=audioDF.sort_values('mid_time'), x='mid_time', y='sentimiento_num', marker='o', color='black')\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
        "plt.title('Sentimiento a lo largo del tiempo (interacción completa)')\n",
        "plt.xlabel('Tiempo (s)')\n",
        "plt.ylabel('Sentimiento')\n",
        "plt.yticks([-1, 0, 1], ['Negativo', 'Neutral', 'Positivo'])\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Ajustar el diseño para que todo sea visible\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"sentimiento_tiempo.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahohwgKlxSUh"
      },
      "source": [
        "###Nube de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahlqHsnyxSUi"
      },
      "outputs": [],
      "source": [
        "text = \" \".join(audioDF['text_clean'].astype(str))\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"wordcloud.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6ZQC5S_xSUi"
      },
      "source": [
        "###Palabras más frecuentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8hriI_IuxSUi"
      },
      "outputs": [],
      "source": [
        "text = \" \".join(audioDF['text_clean'].astype(str)).lower()\n",
        "\n",
        "words = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "word_counts = Counter(words)\n",
        "\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(*zip(*most_common_words))\n",
        "plt.xlabel(\"Palabras\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.title(\"Las 20 palabras más frecuentes\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.savefig(\"palabras_frecuentes.png\", format=\"png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audioDF = audioDF.rename(columns={\n",
        "    \"speaker\": \"orador\",\n",
        "    \"duration\": \"duracion (seg)\",\n",
        "    \"text\": \"texto\"\n",
        "})"
      ],
      "metadata": {
        "id": "WaLSx983EpdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg39WPqLemPR"
      },
      "source": [
        "## Reporte final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iboEdEbDs4bz"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias necesarias\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!apt-get install -y wkhtmltopdf\n",
        "!pip install pdfkit\n",
        "\n",
        "import pdfkit\n",
        "from google.colab import files\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0WhNcB5rBni"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import pdfkit\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Función para convertir imagen a base64\n",
        "def img_to_base64(img_path):\n",
        "    with open(img_path, \"rb\") as img_file:\n",
        "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# Configurar la ruta de wkhtmltopdf\n",
        "config = pdfkit.configuration(wkhtmltopdf=\"/usr/bin/wkhtmltopdf\")\n",
        "\n",
        "html_audioDF = audioDF[[\"orador\",\"duracion (seg)\",\"texto\",\"emocion\",\"sentimiento\"]].to_html(index=False)\n",
        "\n",
        "# Lista de imágenes esperadas\n",
        "imagenes = [\n",
        "    \"emocion_predominante_orador.png\",\n",
        "    \"sentimiento_predominante_orador.png\",\n",
        "    \"emocion_predominante_toda_interaccion.png\",\n",
        "    \"sentimiento_predominante_toda_interaccion.png\",\n",
        "    \"wordcloud.png\",\n",
        "    \"palabras_frecuentes.png\",\n",
        "    \"sentimiento_tiempo_speakers.png\",\n",
        "    \"sentimiento_tiempo.png\"\n",
        "]\n",
        "\n",
        "# Convertir a base64\n",
        "imagenes_base64 = {img: img_to_base64(img) for img in imagenes}\n",
        "\n",
        "# HTML Final\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"es\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <title>Analizador Automático de Sentimientos - Informe</title>\n",
        "    <style>\n",
        "        .page-break {{\n",
        "            page-break-before: always;\n",
        "        }}\n",
        "\n",
        "        body {{\n",
        "            font-family: 'Arial', sans-serif;\n",
        "            margin: 40px;\n",
        "        }}\n",
        "\n",
        "        h1 {{\n",
        "            color: #18181b;\n",
        "            font-size: 1.5rem;\n",
        "            text-align: center;\n",
        "            margin-bottom: 30px;\n",
        "        }}\n",
        "\n",
        "        .flex-row {{\n",
        "            display: flex;\n",
        "            flex-direction: row;\n",
        "            justify-content: space-between;\n",
        "            margin-bottom: 20px;\n",
        "        }}\n",
        "\n",
        "        .flex-col {{\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            gap: 20px;\n",
        "            align-items: center;\n",
        "        }}\n",
        "\n",
        "        .graphs-row {{\n",
        "            max-width: 48%;\n",
        "            height: auto;\n",
        "        }}\n",
        "\n",
        "        .graphs-col {{\n",
        "            max-width: 70%;\n",
        "            height: auto;\n",
        "        }}\n",
        "\n",
        "        table {{\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "            margin-top: 20px;\n",
        "            font-size: 10px;  /* Reducir el tamaño de la fuente */\n",
        "        }}\n",
        "        th, td {{\n",
        "            padding: 6px;  /* Reducir el padding */\n",
        "            text-align: left;\n",
        "            border: 1px solid #ccc;\n",
        "        }}\n",
        "        th {{\n",
        "            background-color: #f4f4f4;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Analizador de Sentimientos y Emociones - Informe</h1>\n",
        "\n",
        "    <div class=\"flex-row\">\n",
        "        <img class=\"graphs-row\" src=\"data:image/png;base64,{imagenes_base64['sentimiento_predominante_orador.png']}\"/>\n",
        "        <img class=\"graphs-row\" src=\"data:image/png;base64,{imagenes_base64['emocion_predominante_orador.png']}\"/>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"flex-row\">\n",
        "        <img class=\"graphs-row\" src=\"data:image/png;base64,{imagenes_base64['sentimiento_predominante_toda_interaccion.png']}\"/>\n",
        "        <img class=\"graphs-row\" src=\"data:image/png;base64,{imagenes_base64['emocion_predominante_toda_interaccion.png']}\"/>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"flex-row\">\n",
        "        <img class=\"graphs-row\" src=\"data:image/png;base64,{imagenes_base64['wordcloud.png']}\"/>\n",
        "        <img class=\"graphs-row\" src=\"data:image/png;base64,{imagenes_base64['palabras_frecuentes.png']}\"/>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"page-break\"></div>\n",
        "\n",
        "    <div class=\"flex-col\">\n",
        "        <img class=\"graphs-col\" src=\"data:image/png;base64,{imagenes_base64['sentimiento_tiempo_speakers.png']}\" />\n",
        "        <img class=\"graphs-col\" src=\"data:image/png;base64,{imagenes_base64['sentimiento_tiempo.png']}\"/>\n",
        "    </div>\n",
        "\n",
        "    <h2 style=\"margin-top: 2rem;\">Tabla de Datos Analizados</h2>\n",
        "    <div style=\"width: 100%;\">\n",
        "        {html_audioDF}\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Configuración de wkhtmltopdf\n",
        "config = pdfkit.configuration(wkhtmltopdf=\"/usr/bin/wkhtmltopdf\")\n",
        "\n",
        "# Guardar PDF\n",
        "pdf_path = \"/content/informe_AS-EC.pdf\"\n",
        "pdfkit.from_string(html_content, pdf_path, configuration=config)\n",
        "\n",
        "# Descargar en Colab\n",
        "files.download(pdf_path)"
      ]
    }
  ]
}